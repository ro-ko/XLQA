# XLQA: A Benchmark for Locale-Aware Multilingual Open-Domain Question Answering

[![EMNLP 2025](https://img.shields.io/badge/EMNLP-2025-blue)](https://2025.emnlp.org/)  
ğŸ“„ Paper (coming soon): [EMNLP 2025 Proceedings](https://arxiv.org/abs/2508.16139))  
ğŸ“ Dataset on Hugging Face: [RO-KO/xlqa](https://huggingface.co/datasets/your-org/xlqa)  
ğŸ“¬ Contact: ro_keonwoo@korea.ac.kr
---

## ğŸ” Overview
 
**XLQA** is a benchmark for evaluating open-domain QA systems in **multilingual** and **locale-aware** settings.

The dataset and code are coming soon!

Key features:

- ğŸŒ Covers 8+ languages and locales
- ğŸ“Œ Emphasizes culturally and geographically grounded knowledge
- âŒ Avoids simple translation-based QA

---

## ğŸ“š Citation

```bibtex
@inproceedings{your2025xlqa,
  title     = {XLQA: A Benchmark for Locale-Aware Multilingual Open-Domain Question Answering},
  author    = {Keon-WuÎ© Roh and Yeong-Joon Ju and Seong-Whan Lee},
  booktitle = {Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2025}
}
```

---

## ğŸ¤ Contributing

We welcome contributions for:

* New locales or languages
* Additional evaluation scripts
* Error corrections and suggestions

Open an issue or pull request!

---

## ğŸ“¬ Contact

* Email: [ro_keonwoo@korea.ac.kr](mailto:ro_keonwoo@korea.ac.kr)
* Issues: [Open a GitHub issue](https://github.com/ro-ko/xlqa/issues)

---

## ğŸ§  Acknowledgements

We thank the multilingual NLP community, annotators, and contributors who made this benchmark possible.
